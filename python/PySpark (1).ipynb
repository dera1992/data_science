{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1G894WS7ltIUTusWWmsCnF_zQhQqZCDOc","timestamp":1669835870722}],"toc_visible":true,"collapsed_sections":["NZ5xsdWmNOVz"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iox_ufgbqDXa"},"source":["<h1><center>Introduction to PySpark</center></h1>"]},{"cell_type":"markdown","metadata":{"id":"FFnYZltvqLgt"},"source":["<a id='objective'></a>\n","## Objective\n","The objective of this notebook is to:\n","><li>Give a proper understanding about the different PySpark functions available. </li>\n","><li>A short introduction to Google Colab, as that is the platform on which this notebook is written on. </li>\n","\n","Once you complete this notebook, you should be able to write pyspark programs in an efficent way. The ideal way to use this is by going through the examples given and then trying them on Colab. At the end there are a few hands on questions which you can use to evaluate yourself."]},{"cell_type":"markdown","metadata":{"id":"K_q3Yzc9qYc0"},"source":["<a id='pyspark-and-colaboratory'></a>\n","## PySpark and Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"NhM3wLG2qhlN"},"source":["<a id='pyspark'></a>\n","### PySpark"]},{"cell_type":"markdown","metadata":{"id":"VBfC_kvjqjPj"},"source":["If you are working in the field of big data, you must have definelty heard of spark. If you look at the [Apache Spark](https://spark.apache.org/) website, you will see that it is said to be a `Lightning-fast unified analytics engine`. PySpark is a flavour of Spark used for processing and analysing massive volumes of data. If you are familiar with python and have tried it for huge datasets, you should know that the execution time can get ridiculous. Enter PySpark!\n","\n","Imagine your data resides in a distributed manner at different places. If you try brining your data to one point and executing your code there, not only would that be inefficent, but also cause memory issues. Now let's say your code goes to the data rather than the data coming to where your code. This will help avoid unneccesary data movement which will thereby decrease the running time. \n","\n","PySpark is the Python API of Spark; which means it can do almost all the things python can. Machine learning(ML) pipelines, exploratory data analysis (at scale), ETLs for data platform, and much more! And all of them in a distributed manner. One of the best parts of pyspark is that if you are already familiar with python, it's really easy to learn.\n","\n","Apart from PySpark, there is another language called Scala used for big data processing. Scala is frequently over 10 times faster than *Python*, as it is native for Hadoop as its based on JVM. But PySpark is getting adopted at a fast rate because of the ease of use, easier learning curve and ML capabilities.\n","\n","I will briefly explain how a PySpark job works, but I strongly recommend you read more about the [architecture](https://data-flair.training/blogs/how-apache-spark-works/) and how everything works. Now, before I get into it, let me talk about some <u>basic jargons</u> first:\n","\n","<b>Cluster</b> is a set of loosely or tightly connected computers that work together so that they can be viewed as a single system.\n","\n","<b>Hadoop</b> is an open source, scalable, and fault tolerant framework written in Java. It efficiently processes large volumes of data on a cluster of commodity hardware. Hadoop is not only a storage system but is a platform for large data storage as well as processing.\n","\n","<b>HDFS</b> (Hadoop distributed file system). It is one of the world's most reliable storage system. HDFS is a Filesystem of Hadoop designed for storing very large files running on a cluster of commodity hardware.\n","\n","<b>MapReduce</b> is a data Processing framework, which has 2 phases - Mapper and Reducer. The map procedure performs filtering and sorting, and the reduce method performs a summary operation. It usually runs on a hadoop cluster.\n","\n","<b>Transformation</b> refers to the operations applied on a dataset to create a new dataset. Filter, groupBy and map are the examples of transformations.\n","\n","<b>Actions</b> Actions refer to an operation which instructs Spark to perform computation and send the result back to driver. This is an example of action.\n","\n","Alright! Now that that's out of the way, let me explain how a spark job runs. In simple terma, each time you submit a pyspark job, the code gets internally converted into a MapReduce program and gets executed in the Java virtual machine. Now one of the thoughts that might be popping in your mind will probably be: <br>`So the code gets converted into a MapReduce program. Wouldn't that mean MapReduce is faster than pySpark?`<br> Well, the answer is a big NO. This is what makes spark jobs special. Spark is capable of handling a massive amount of data at a time, in it's distributed environment. It does this through <u>in-memory processing</u>, which is what makes it almost 100 times faster than Hadoop. Another factor which amkes it fast is <u>Lazy Evaluation</u>. Spark delays its evaluation as much as it can. Each time you  submit a job, spark creates an action plan for how to execute the code, and then does nothing. Finally, when you ask for the result(i.e, calls an action), it executes the plan, which is basically all the transofrmations you have mentioned in your code. That's basically the gist of it. \n","\n","Now lastly, I want to talk about on more thing. Spark mainly consists of 4 modules:\n","\n","<ol>\n","    <li>Spark SQL - helps to write  spark programs using SQL like queries.</li>\n","    <li>Spark Streaming - is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. used heavily in processing of social media data.</li>\n","    <li>Spark MLLib - is the machine learning component of SPark. It helps train ML models on massive datasets with very high efficeny. </li>\n","    <li>Spark GraphX - is the visualization component of Spark. It enables users to view data both as graphs and as collections without data movement or duplication.</li>\n","</ol>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1NJMWs4NqnlF"},"source":["<a id='colaboratory'></a>\n","### Colaboratory"]},{"cell_type":"markdown","metadata":{"id":"ynKFk6b7qoWr"},"source":["In the words of Google: <br>\n","`Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.`\n","\n","The reason why I used colab is because of its shareability and free GPU and TPU. Yeah you read that right, FREE GPU AND TPU! For using TPU, your program needs to be optimized for the same. Additionally, it helps use different Google services conveniently. It saves to Google Drive and all the services are very closely related. I recommend you go through the offical [overview documentation](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) if you want to know more about it.\n","If you have more questions about colab, please [refer this link](https://research.google.com/colaboratory/faq.html).\n","\n",">While using a colab notebook, you will need an active internet connection to keep a session alive. If you lose the connection you will have to download the datasets again."]},{"cell_type":"markdown","metadata":{"id":"_N5-lspH_N8B"},"source":["<a id='jupyter-notebook-basics'></a>\n","## Jupyter notebook basics"]},{"cell_type":"markdown","metadata":{"id":"6Ul54hAYyHyd"},"source":["<a id='code-cells'></a>\n","### Code cells"]},{"cell_type":"code","metadata":{"id":"j38beRUTCI5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669836782002,"user_tz":0,"elapsed":421,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}},"outputId":"43ea6c9b-7937-4810-b2e0-dccdc1732972"},"source":["2*3"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"_Jewe_e9CIYa","executionInfo":{"status":"ok","timestamp":1669836786283,"user_tz":0,"elapsed":187,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}}},"source":["from collections import Counter"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8Y7w6_CCIIT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669836790391,"user_tz":0,"elapsed":198,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}},"outputId":"c08cfe28-ace4-41fa-c56c-48cf0cc26a4e"},"source":["print(\"This is a tutorial!\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a tutorial!\n"]}]},{"cell_type":"markdown","metadata":{"id":"VOqLNkRKyUIS"},"source":["<a id='text-cells'></a>\n","### Text cells"]},{"cell_type":"markdown","metadata":{"id":"IfbaUe-oq7DK"},"source":["Hello world!"]},{"cell_type":"markdown","metadata":{"id":"X6zdrH15_CCW"},"source":["<a id='access-to-the-shell'></a>\n","### Access to the shell"]},{"cell_type":"markdown","source":["- To check the files and directories in current work place"],"metadata":{"id":"p_t22RcCCGW5"}},{"cell_type":"code","metadata":{"id":"zdO9sjSdEVnr"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To check the current working folder"],"metadata":{"id":"bhn1UlyvCSJr"}},{"cell_type":"code","metadata":{"id":"QF9e3lDDEX3I"},"source":["pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dd6t0uFzuR4X"},"source":["<a id='installing-spark'></a>\n","### Installing Spark"]},{"cell_type":"markdown","metadata":{"id":"6apGVff5h4ca"},"source":["Install Dependencies:\n","\n","\n","1.   Java 8\n","2.   Apache Spark with hadoop and\n","3.   Findspark (used to locate the spark in the system)\n"]},{"cell_type":"code","metadata":{"id":"tt7ZS1_wGgjn","executionInfo":{"status":"ok","timestamp":1669837005979,"user_tz":0,"elapsed":33139,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}}},"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n","!pip install -q findspark"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3x0ZRLxjMVr"},"source":["Set Environment Variables:"]},{"cell_type":"code","metadata":{"id":"sdOOq4twHN1K","executionInfo":{"status":"aborted","timestamp":1669836758473,"user_tz":0,"elapsed":9,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ACYMwhgHTYz","executionInfo":{"status":"aborted","timestamp":1669836758473,"user_tz":0,"elapsed":9,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}}},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KR1zLBk1998Z","executionInfo":{"status":"aborted","timestamp":1669836758473,"user_tz":0,"elapsed":844,"user":{"displayName":"Pradeep Hewage","userId":"06125653909624903571"}}},"source":["import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmIqq6xPK7m7"},"source":["<a id='exploring-the-dataset'></a>\n","## Exploring the Dataset"]},{"cell_type":"markdown","metadata":{"id":"VZwsr57lwPgq"},"source":["<a id='loading-the-dataset'></a>\n","### Loading the Dataset"]},{"cell_type":"code","metadata":{"id":"hQ3zmGACLKlN"},"source":["# Downloading and preprocessing Cars Data downloaded origianlly from https://perso.telecom-paristech.fr/eagan/class/igr204/datasets\n","!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wpq2jYvIMOJy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617128773996,"user_tz":-330,"elapsed":1049,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"a85b230e-13e6-4d29-8521-de5c99a689b9"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cars.csv  sample_data  spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hz6ALr5mMqZt"},"source":["# Load data from csv to a dataframe. \n","# header=True means the first row is a header \n","# sep=';' means the column are seperated using ''\n","df = spark.read.csv('cars.csv', header=True, sep=\";\")\n","df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0lCS2LNwnoy"},"source":["The above command loads our data from into a dataframe (DF). A dataframe is a 2-dimensional labeled data structure with columns of potentially different types."]},{"cell_type":"markdown","metadata":{"id":"_QwZtWxZRCBn"},"source":["<a id='viewing-the-dataframe'></a>\n","### Viewing the Dataframe"]},{"cell_type":"markdown","metadata":{"id":"50LZ3S8_PMg_"},"source":["There are a couple of ways to view your dataframe(DF) in PySpark:\n","\n","1.   `df.take(5)` will return a list of five Row objects. \n","2.   `df.collect()` will get all of the data from the entire DataFrame. Be really careful when using it, because if you have a large data set, you can easily crash the driver node. \n","3.   `df.show()` is the most commonly used method to view a dataframe. There are a few parameters we can pass to this method, like the number of rows and truncaiton. For example, `df.show(5, False)` or ` df.show(5, truncate=False)` will show the entire data wihtout any truncation.\n","4.   `df.limit(5)` will **return a new DataFrame** by taking the first n rows. As spark is distributed in nature, there is no guarantee that `df.limit()` will give you the same results each time.\n","\n","Let us see some of them in action below:"]},{"cell_type":"code","metadata":{"id":"I1qqkqcfxM0v"},"source":["df.show(5, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9zwzswIxXF9"},"source":["df.limit(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUazdCEmu_sp"},"source":["<a id='viewing-dataframe-columns'></a>\n","### Viewing Dataframe Columns"]},{"cell_type":"code","metadata":{"id":"9o7jsazcu-13"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lfS2DhHuhPl"},"source":["<a id='dataframe-schema'></a>\n","### Dataframe Schema"]},{"cell_type":"markdown","metadata":{"id":"-xX7hRoW_cXY"},"source":["There are two methods commonly used to view the data types of a dataframe:"]},{"cell_type":"code","metadata":{"id":"w6qwTjGsNxrw"},"source":["df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCGTFlCWRPw4"},"source":["df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXx5ATpZ9oor"},"source":["<a id='implicit-schema-inference'></a>\n","#### Inferring Schema Implicitly"]},{"cell_type":"markdown","metadata":{"id":"7TeflTUp8l29"},"source":["We can use the parameter `inferschema=true` to infer the input schema automatically while loading the data. An example is shown below:"]},{"cell_type":"code","metadata":{"id":"Qym5MjCi894N"},"source":["df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6jTedYd-Dhb"},"source":["As you can see, the datatype has been infered automatically spark with even the correct precison for decimal type. A problem that might arise here is that sometimes, when you have to read multiple files with different schemas in different files, there might be an issue with implicit inferring leading to null values in some columns. Therefore, let us also see how to define schemas explicitly."]},{"cell_type":"markdown","metadata":{"id":"yTVjYqeRuxWn"},"source":["<a id='explicit-schema-inference'></a>\n","#### Defining Schema Explicitly"]},{"cell_type":"code","metadata":{"id":"xpsaQ4JMRUiS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617128817171,"user_tz":-330,"elapsed":1092,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"cbfbeed2-7aa1-43de-c8a6-5d505c164062"},"source":["from pyspark.sql.types import *\n","df.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Car',\n"," 'MPG',\n"," 'Cylinders',\n"," 'Displacement',\n"," 'Horsepower',\n"," 'Weight',\n"," 'Acceleration',\n"," 'Model',\n"," 'Origin']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"ik62VX34SlFh"},"source":["# Creating a list of the schema in the format column_name, data_type\n","labels = [\n","     ('Car',StringType()),\n","     ('MPG',DoubleType()),\n","     ('Cylinders',IntegerType()),\n","     ('Displacement',DoubleType()),\n","     ('Horsepower',DoubleType()),\n","     ('Weight',DoubleType()),\n","     ('Acceleration',DoubleType()),\n","     ('Model',IntegerType()),\n","     ('Origin',StringType())\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-Fp5y_oU9SF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617128821296,"user_tz":-330,"elapsed":1216,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"a0a84e9b-3cca-431f-ba19-d3c6c183c797"},"source":["# Creating the schema that will be passed when reading the csv\n","schema = StructType([StructField (x[0], x[1], True) for x in labels])\n","schema"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["StructType(List(StructField(Car,StringType,true),StructField(MPG,DoubleType,true),StructField(Cylinders,IntegerType,true),StructField(Displacement,DoubleType,true),StructField(Horsepower,DoubleType,true),StructField(Weight,DoubleType,true),StructField(Acceleration,DoubleType,true),StructField(Model,IntegerType,true),StructField(Origin,StringType,true)))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"sgC7gtL5VTls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617128823257,"user_tz":-330,"elapsed":956,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"1a84e4b5-0c32-4d8f-f399-602a09156346"},"source":["df = spark.read.csv('cars.csv', header=True, sep=\";\", schema=schema)\n","df.printSchema()\n","# The schema comes as we gave!"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- Car: string (nullable = true)\n"," |-- MPG: double (nullable = true)\n"," |-- Cylinders: integer (nullable = true)\n"," |-- Displacement: double (nullable = true)\n"," |-- Horsepower: double (nullable = true)\n"," |-- Weight: double (nullable = true)\n"," |-- Acceleration: double (nullable = true)\n"," |-- Model: integer (nullable = true)\n"," |-- Origin: string (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dn2EAhesVmx0"},"source":["df.show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDCO3TEe95OY"},"source":["As we can see here, the data has been successully loaded with the specified datatypes."]},{"cell_type":"markdown","metadata":{"id":"rsD48rckdHPe"},"source":["<a id='dataframe-operations-on-columns'></a>\n","## DataFrame Operations on Columns"]},{"cell_type":"markdown","metadata":{"id":"cMlxdWfSY8ks"},"source":["We will go over the following in this section:\n","\n","1.   Selecting Columns\n","2.   Selecting Multiple Columns\n","3.   Adding New Columns\n","4.   Renaming Columns\n","5.   Grouping By Columns\n","6.   Removing Columns\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ikGR5pDICTu7"},"source":["<a id='selecting-columns'></a>\n","### Selecting Columns"]},{"cell_type":"markdown","metadata":{"id":"-VMwIwi2rj_o"},"source":["There are multiple ways to do a select in PySpark. You can find how they differ and how each below:"]},{"cell_type":"code","metadata":{"id":"ge9-_ygideWk"},"source":["# 1st method\n","# Column name is case sensitive in this usage\n","print(df.Car)\n","print(\"*\"*20)\n","df.select(df.Car).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxP1su8veNde"},"source":["**NOTE:**\n","\n","> **We can't always use the dot notation because this will break when the column names have reserved names or attributes to the data frame class. Additionally, the column names are case sensitive in nature so we need to always make sure the column names have been changed to a paticular case before using it.**\n","\n"]},{"cell_type":"code","metadata":{"id":"md5zaET8dsr4"},"source":["# 2nd method\n","# Column name is case insensitive here\n","print(df['car'])\n","print(\"*\"*20)\n","df.select(df['car']).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Gkf14sHec9a"},"source":["# 3rd method\n","# Column name is case insensitive here\n","from pyspark.sql.functions import col\n","df.select(col('car')).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6QsMfnNt3qF"},"source":["<a id='selecting-multiple-columns'></a>\n","### Selecting Multiple Columns"]},{"cell_type":"code","metadata":{"id":"bPjLMhZ6uAQR"},"source":["# 1st method\n","# Column name is case sensitive in this usage\n","print(df.Car, df.Cylinders)\n","print(\"*\"*40)\n","df.select(df.Car, df.Cylinders).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMRMUrv7uHWa"},"source":["# 2nd method\n","# Column name is case insensitive in this usage\n","print(df['car'],df['cylinders'])\n","print(\"*\"*40)\n","df.select(df['car'],df['cylinders']).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgQ20-4GugjR"},"source":["# 3rd method\n","# Column name is case insensitive in this usage\n","from pyspark.sql.functions import col\n","df.select(col('car'),col('cylinders')).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85Lv3zSXCcOY"},"source":["<a id='adding-new-columns'></a>\n","### Adding New Columns"]},{"cell_type":"markdown","metadata":{"id":"d_Y7dcAHu-Uz"},"source":["We will take a look at three cases here:\n","\n","1.   Adding a new column\n","2.   Adding multiple columns\n","3.   Deriving a new column from an exisitng one"]},{"cell_type":"code","metadata":{"id":"oFHUmRKZeCEV"},"source":["# CASE 1: Adding a new column\n","# We will add a new column called 'first_column' at the end\n","from pyspark.sql.functions import lit\n","df = df.withColumn('first_column',lit(1)) \n","# lit means literal. It populates the row with the literal value given.\n","# When adding static data / constant values, it is a good practice to use it.\n","df.show(5,truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9772_mHwAqL"},"source":["# CASE 2: Adding multiple columns\n","# We will add two new columns called 'second_column' and 'third_column' at the end\n","df = df.withColumn('second_column', lit(2)) \\\n","       .withColumn('third_column', lit('Third Column')) \n","# lit means literal. It populates the row with the literal value given.\n","# When adding static data / constant values, it is a good practice to use it.\n","df.show(5,truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGaQS_pOwx_b"},"source":["# CASE 3: Deriving a new column from an exisitng one\n","# We will add a new column called 'car_model' which has the value of car and model appended together with a space in between \n","from pyspark.sql.functions import concat\n","df = df.withColumn('car_model', concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n","# lit means literal. It populates the row with the literal value given.\n","# When adding static data / constant values, it is a good practice to use it.\n","df.show(5,truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xeHExg-zxf5r"},"source":["As we can see, the new column car model has been created from existing columns. Since our aim was to create a column which has the value of car and model appended together with a space in between we have used the `concat` operator."]},{"cell_type":"markdown","metadata":{"id":"QlMf04i2CjDC"},"source":["<a id='renaming-columns'></a>\n","### Renaming Columns"]},{"cell_type":"markdown","metadata":{"id":"CwGKbSHvxxxG"},"source":["We use the `withColumnRenamed` function to rename a columm in PySpark. Let us see it in action below:"]},{"cell_type":"code","metadata":{"id":"QJqgy6lKfk2o"},"source":["#Renaming a column in PySpark\n","df = df.withColumnRenamed('first_column', 'new_column_one') \\\n","       .withColumnRenamed('second_column', 'new_column_two') \\\n","       .withColumnRenamed('third_column', 'new_column_three')\n","df.show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CDifVC2Cnml"},"source":["<a id='grouping-by-columns'></a>\n","### Grouping By Columns"]},{"cell_type":"markdown","metadata":{"id":"9wlB76FdyS0W"},"source":["Here, we see the Dataframe API way of grouping values. We will discuss how to:\n","\n","\n","1.   Group By a single column\n","2.   Group By multiple columns"]},{"cell_type":"code","metadata":{"id":"M1ek2opVfqea"},"source":["# Group By a column in PySpark\n","df.groupBy('Origin').count().show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUh_TWcOysoL"},"source":["# Group By multiple columns in PySpark\n","df.groupBy('Origin', 'Model').count().show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CbpEj9fECrW3"},"source":["<a id='removing-columns'></a>\n","### Removing Columns"]},{"cell_type":"code","metadata":{"id":"xsb9PXxpfnmh"},"source":["#Remove columns in PySpark\n","df = df.drop('new_column_one')\n","df.show(5,truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKOXrXtvzK_0"},"source":["#Remove multiple columnss in one go\n","df = df.drop('new_column_two') \\\n","       .drop('new_column_three')\n","df.show(5,truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbKK5iHwmIoV"},"source":["<a id='dataframe-operations-on-rows'></a>\n","## DataFrame Operations on Rows"]},{"cell_type":"markdown","metadata":{"id":"Quwx3KlLzeq9"},"source":["We will discuss the follwoing in this section:\n","\n","1.   Filtering Rows\n","2. \t Get Distinct Rows\n","3.   Sorting Rows\n","4.   Union Dataframes\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9bKlvX-SH-Wy"},"source":["<a id='filtering-rows'></a>\n","### Filtering Rows"]},{"cell_type":"code","metadata":{"id":"YNfcjOIknA3n"},"source":["# Filtering rows in PySpark\n","total_count = df.count()\n","print(\"TOTAL RECORD COUNT: \" + str(total_count)) \n","europe_filtered_count = df.filter(col('Origin')=='Europe').count()\n","print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n","df.filter(col('Origin')=='Europe').show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXJxRwBQ1lyd"},"source":["# Filtering rows in PySpark based on Multiple conditions\n","total_count = df.count()\n","print(\"TOTAL RECORD COUNT: \" + str(total_count)) \n","europe_filtered_count = df.filter((col('Origin')=='Europe') & \n","                                  (col('Cylinders')==4)).count() # Two conditions added here\n","print(\"EUROPE FILTERED RECORD COUNT: \" + str(europe_filtered_count))\n","df.filter(col('Origin')=='Europe').show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLU-a4auIEvh"},"source":["<a id='get-distinct-rows'></a>\n","### Get Distinct Rows"]},{"cell_type":"code","metadata":{"id":"B1RKg1UrmBQz"},"source":["#Get Unique Rows in PySpark\n","df.select('Origin').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_LQWXPXt0g0N"},"source":["#Get Unique Rows in PySpark based on mutliple columns\n","df.select('Origin','model').distinct().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-069UYUwIIYI"},"source":["<a id='sorting-rows'></a>\n","### Sorting Rows"]},{"cell_type":"code","metadata":{"id":"4ZpeJvz0nkBI"},"source":["# Sort Rows in PySpark\n","# By default the data will be sorted in ascending order\n","df.orderBy('Cylinders').show(truncate=False) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1CEwofMJV-D"},"source":["# To change the sorting order, you can use the ascending parameter\n","df.orderBy('Cylinders', ascending=False).show(truncate=False) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zx3W4aeL5A4O"},"source":["# Using groupBy aand orderBy together\n","df.groupBy(\"Origin\").count().orderBy('count', ascending=False).show(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aN0-A_JsIX-X"},"source":["<a id='union-dataframes'></a>\n","### Union Dataframes"]},{"cell_type":"markdown","metadata":{"id":"VH0KOaBrJt6v"},"source":["You will see three main methods for performing union of dataframes. It is important to know the difference between them and which one is preferred:\n","\n","*   `union()` – It is used to merge two DataFrames of the same structure/schema. If schemas are not the same, it returns an error\n","*   `unionAll()` – This function is deprecated since Spark 2.0.0, and replaced with union()\n","*   `unionByName()` - This function is used to merge two dataframes based on column name.\n","\n","> Since `unionAll()` is deprecated, **`union()` is the preferred method for merging dataframes.**\n","<br>\n","> The difference between `unionByName()` and `union()` is that `unionByName()` resolves columns by name, not by position.\n","\n","In other SQLs, Union eliminates the duplicates but UnionAll merges two datasets, thereby including duplicate records. But, in PySpark, both behave the same and includes duplicate records. The recommendation is to use `distinct()` or `dropDuplicates()` to remove duplicate records."]},{"cell_type":"code","metadata":{"id":"bCZIzfYmnx--"},"source":["# CASE 1: Union When columns are in order\n","df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)\n","europe_cars = df.filter((col('Origin')=='Europe') & (col('Cylinders')==5))\n","japan_cars = df.filter((col('Origin')=='Japan') & (col('Cylinders')==3))\n","print(\"EUROPE CARS: \"+str(europe_cars.count()))\n","print(\"JAPAN CARS: \"+str(japan_cars.count()))\n","print(\"AFTER UNION: \"+str(europe_cars.union(japan_cars).count()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pfPzVOFqC_8"},"source":["**Result:**\n","\n","> As you can see here, there were 3 cars from Europe with 5 Cylinders, and 4 cars from Japan with 3 Cylinders. After union, there are 7 cars in total.\n","\n"]},{"cell_type":"code","metadata":{"id":"CjWjzWBoMxx0"},"source":["# CASE 1: Union When columns are not in order\n","# Creating two dataframes with jumbled columns\n","df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n","df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n","df1.unionByName(df2).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EX33t8e3PyGy"},"source":["**Result:**\n","\n","> As you can see here, the two dataframes have been successfully merged based on their column names.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aHjILb1DriuX"},"source":["<a id='common-data-manipulation-functions'></a>\n","## Common Data Manipulation Functions"]},{"cell_type":"code","metadata":{"id":"x3vlC7ZerlKb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617128999118,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"89d386c3-e9c2-4532-c086-f8b00cc38881"},"source":["# Functions available in PySpark\n","from pyspark.sql import functions\n","# Similar to python, we can use the dir function to view the avaiable functions\n","print(dir(functions)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_udf', '_get_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_column', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aggregate', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'array_distinct', 'array_except', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bitwiseNOT', 'broadcast', 'bround', 'bucket', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'filter', 'first', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hours', 'hypot', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_concat', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'nanvl', 'next_day', 'nth_value', 'ntile', 'overlay', 'pandas_udf', 'percent_rank', 'percentile_approx', 'posexplode', 'posexplode_outer', 'pow', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'second', 'sequence', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shuffle', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sys', 'tan', 'tanh', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'xxhash64', 'year', 'years', 'zip_with']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PIKigra7A34e"},"source":["<a id='string-functions'></a>\n","### String Functions"]},{"cell_type":"code","metadata":{"id":"63QDccSjBqC4"},"source":["# Loading the data\n","from pyspark.sql.functions import col\n","df = spark.read.csv('cars.csv', header=True, sep=\";\", inferSchema=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LiXWN8DUA9x6"},"source":["**Display the Car column in exisitng, lower and upper characters, and the first 4 characters of the column**"]},{"cell_type":"code","metadata":{"id":"52Gh9c99BZFr"},"source":["from pyspark.sql.functions import col,lower, upper, substring\n","# Prints out the details of a function\n","help(substring)\n","# alias is used to rename the column name in the output\n","df.select(col('Car'),lower(col('Car')),upper(col('Car')),substring(col('Car'),1,4).alias(\"concatenated value\")).show(5, False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZJFdTbk6rBt"},"source":["**Concatenate the Car column and Model column and add a space between them.**"]},{"cell_type":"code","metadata":{"id":"8Lo951Cg6phi"},"source":["from pyspark.sql.functions import concat\n","df.select(col(\"Car\"),col(\"model\"),concat(col(\"Car\"), lit(\" \"), col(\"model\"))).show(5, False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldtA0wk9BMkT"},"source":["<a id='numeric-functions'></a>\n","### Numeric functions"]},{"cell_type":"markdown","metadata":{"id":"kmz4G5LVBOs6"},"source":["**Show the oldest date and the most recent date**"]},{"cell_type":"code","metadata":{"id":"wBDDH-YpBbdk"},"source":["from pyspark.sql.functions import min, max\n","df.select(min(col('Weight')), max(col('Weight'))).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTg-Royz7Nvi"},"source":["**Add 10 to the minimum and maximum weight**"]},{"cell_type":"code","metadata":{"id":"YeiemMsI7Vm2"},"source":["from pyspark.sql.functions import min, max, lit\n","df.select(min(col('Weight'))+lit(10), max(col('Weight')+lit(10))).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQ6Ul9HGCwC3"},"source":["<a id='operations-on-date'></a>\n","### Operations on Date"]},{"cell_type":"markdown","metadata":{"id":"s1jmBN2qFHyk"},"source":["> [PySpark follows SimpleDateFormat table of Java. Click here to view the docs.](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html)"]},{"cell_type":"code","metadata":{"id":"sCTeI_JvDCsH"},"source":["from pyspark.sql.functions import to_date, to_timestamp, lit\n","df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n","df.show()\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZH8ja1eHEW8x"},"source":["df = spark.createDataFrame([('2019-12-25 13:30:00',)], ['DOB'])\n","df = df.select(to_date(col('DOB'),'yyyy-MM-dd HH:mm:ss'), to_timestamp(col('DOB'),'yyyy-MM-dd HH:mm:ss'))\n","df.show()\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7g9m_8PPErI1"},"source":["df = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['DOB'])\n","df = df.select(to_date(col('DOB'),'dd/MMM/yyyy HH:mm:ss'), to_timestamp(col('DOB'),'dd/MMM/yyyy HH:mm:ss'))\n","df.show()\n","df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIPQyQV7-Hz5"},"source":["**What is 3 days earlier that the oldest date and 3 days later than the most recent date?**"]},{"cell_type":"code","metadata":{"id":"PUCEwQkZ-I7h"},"source":["from pyspark.sql.functions import date_add, date_sub\n","# create a dummy dataframe\n","df = spark.createDataFrame([('1990-01-01',),('1995-01-03',),('2021-03-30',)], ['Date'])\n","# find out the required dates\n","df.select(date_add(max(col('Date')),3), date_sub(min(col('Date')),3)).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7OZElEvcGOD1"},"source":["<a id='joins-in-pyspark'></a>\n","## Joins in PySpark"]},{"cell_type":"code","metadata":{"id":"UJBC7r3JFyCL"},"source":["# Create two dataframes\n","cars_df = spark.createDataFrame([[1, 'Car A'],[2, 'Car B'],[3, 'Car C']], [\"id\", \"car_name\"])\n","car_price_df = spark.createDataFrame([[1, 1000],[2, 2000],[3, 3000]], [\"id\", \"car_price\"])\n","cars_df.show()\n","car_price_df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7Py4EYyKJTN"},"source":["# Executing an inner join so we can see the id, name and price of each car in one row\n","cars_df.join(car_price_df, cars_df.id == car_price_df.id, 'inner').select(cars_df['id'],cars_df['car_name'],car_price_df['car_price']).show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vj0mPaHU5i5n"},"source":["As you can see, we have done an inner join between two dataframes. The following joins are supported by PySpark:\n","1. inner (default)\n","2. cross\n","3. outer\n","4. full\n","5. full_outer\n","6. left\n","7. left_outer\n","8. right\n","9. right_outer\n","10. left_semi\n","11. left_anti"]},{"cell_type":"markdown","metadata":{"id":"HNPhsx8P2tUH"},"source":["<a id='spark-sql'></a>\n","## Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"rHMvBBAh23cw"},"source":["SQL has been around since the 1970s, and so one can imagine the number of people who made it their bread and butter. As big data came into popularity, the number of professionals with the technical knowledge to deal with it was in shortage. This led to the creation of Spark SQL. To quote the docs:<br>\n",">Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations.\n","\n","Basically, what you need to know is that Spark SQL is used to execute SQL queries on big data. Spark SQL can also be used to read data from Hive tables and views. Let me explain Spark SQL with an example.\n"]},{"cell_type":"code","metadata":{"id":"g2DaK9-D7QkX"},"source":["# Load data\n","df = spark.read.csv('cars.csv', header=True, sep=\";\")\n","# Register Temporary Table\n","df.createOrReplaceTempView(\"temp\")\n","# Select all data from temp table\n","spark.sql(\"select * from temp limit 5\").show()\n","# Select count of data in table\n","spark.sql(\"select count(*) as total_count from temp\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6i32WE8j_ec8"},"source":["As you can see, we registered the dataframe as temporary table and then ran basic SQL queries on it. How amazing is that?!<br>\n","If you are a person who is more comfortable with SQL, then this feature is truly a blessing for you! But this raises a question: \n","> *Should I just keep using Spark SQL all the time?*\n","\n","And the answer is, _**it depends**_.<br>\n","So basically, the different functions acts in differnet ways, and depending upon the type of action you are trying to do, the speed at which it completes execution also differs. But as time progress, this feature is getting better and better, so hopefully the difference should be a small margin. There are plenty of analysis done on this, but nothing has a definite answer yet. You can read this [comparative study done by horton works](https://community.cloudera.com/t5/Community-Articles/Spark-RDDs-vs-DataFrames-vs-SparkSQL/ta-p/246547) or the answer to this [stackoverflow question](https://stackoverflow.com/questions/45430816/writing-sql-vs-using-dataframe-apis-in-spark-sql) if you are still curious about it."]},{"cell_type":"markdown","metadata":{"id":"x62BiCgBMOtq"},"source":["<a id='rdd'></a>\n","## RDD"]},{"cell_type":"markdown","metadata":{"id":"VGXK6uEuUKRh"},"source":["> With map, you define a function and then apply it record by record. Flatmap returns a new RDD by first applying a function to all of the elements in RDDs and then flattening the result. Filter, returns a new RDD. Meaning only the elements that satisfy a condition. With reduce, we are taking neighboring elements and producing a single combined result.\n","For example, let's say you have a set of numbers. You can reduce this to its sum by providing a function that takes as input two values and reduces them to one. \n","\n","Some of the reasons you would use a dataframe over RDD are:\n","1.   It's ability to represnt data as rows and columns. But this also means it can only hold structred and semi-structured data.\n","2.   It allows processing data in different formats (AVRO, CSV, JSON, and storage system HDFS, HIVE tables, MySQL).\n","3. It's superior job Optimization capability.\n","4. DataFrame API is very easy to use.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0_WvAgyvR7m6"},"source":["cars = spark.sparkContext.textFile('cars.csv')\n","print(cars.first())\n","cars_header = cars.first()\n","cars_rest = cars.filter(lambda line: line!=cars_header)\n","print(cars_rest.first())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P65eAFO3Mkdd"},"source":["**How many cars are there in our csv data?**"]},{"cell_type":"code","metadata":{"id":"Vi03EU0CMSmO"},"source":["cars_rest.map(lambda line: line.split(\";\")).count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3c4bci70MnlQ"},"source":["**Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in Europe**"]},{"cell_type":"code","metadata":{"id":"fWFpo_WxMnvm"},"source":["# Car name is column  0\n","(cars_rest.filter(lambda line: line.split(\";\")[8]=='Europe').\n"," map(lambda line: (line.split(\";\")[0],\n","    line.split(\";\")[1],\n","    line.split(\";\")[2],\n","    line.split(\";\")[5],\n","    line.split(\";\")[8])).collect())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYmb5FscMph3"},"source":["**Display the Car name, MPG, Cylinders, Weight and Origin for the cars Originating in either Europe or Japan**"]},{"cell_type":"code","metadata":{"id":"6ZcRIX3mMquF"},"source":["# Car name is column  0\n","(cars_rest.filter(lambda line: line.split(\";\")[8] in ['Europe','Japan']).\n"," map(lambda line: (line.split(\";\")[0],\n","    line.split(\";\")[1],\n","    line.split(\";\")[2],\n","    line.split(\";\")[5],\n","    line.split(\";\")[8])).collect())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wn2zXe7TbI3"},"source":["<a id='user-defined-functions-udf'></a>\n","## User-Defined Functions (UDF)"]},{"cell_type":"markdown","metadata":{"id":"w0YWspcTRrin"},"source":["PySpark User-Defined Functions (UDFs) help you convert your python code into a scalable version of itself. It comes in handy more than you can imagine, but beware, as the performance is less when you compare it with pyspark functions. You can view examples of how UDF works [here](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html). What I will give in this section is some theory on how it works, and why it is slower.\n","\n","When you try to run a UDF in PySpark, each executor creates a python process. Data will be serialised and deserialised between each executor and python. This leads to lots of performance impact and overhead on spark jobs, making it less efficent than using spark dataframes. Apart from this, sometimes you might have memory issues while using UDFs. The Python worker consumes huge off-heap memory and so it often leads to memoryOverhead, thereby failing your job. Keeping these in mind, I wouldn't recommend using them, but at the end of the day, your choice."]},{"cell_type":"markdown","metadata":{"id":"yv7ODDTQRwVt"},"source":["<a id='common-questions'></a>\n","# Common Questions"]},{"cell_type":"markdown","metadata":{"id":"VZ1bYvF8R8Dc"},"source":["<a id='submitting-a-spark-job'></a>\n","## Submitting a Spark Job"]},{"cell_type":"markdown","metadata":{"id":"3EQWnq23SCbE"},"source":["The python syntax for running jobs is: `python <file_name>.py <arg1> <arg2> ...`\n","<br>But when you submit a spark job you have to use spark-submit to run the application.\n","\n","Here is a simple example of a spark-submit command:\n","`spark-submit filename.py --named_argument 'arguemnt value'`<br>\n","Here, named_argument is an argument that you are reading from inside your script.\n","\n","There are other options you can pass in the command, like:<br>\n","`--py-files` which helps you pass a python file to read in your file,<br>\n","`--files` which helps pass other files like txt or config,<br>\n","`--deploy-mode` which tells wether to deploy your worker node on cluster or locally <br>\n","`--conf` which helps pass different configurations, like memoryOverhead, dynamicAllocation etc.\n","\n","There is an [entire page](https://spark.apache.org/docs/latest/submitting-applications.html) in spark documentation dedicated to this. I highly recommend you go through it once."]},{"cell_type":"markdown","metadata":{"id":"oVwGYAZZiyGV"},"source":["<a id='creating-dataframes'></a>\n","## Creating Dataframes"]},{"cell_type":"markdown","metadata":{"id":"TvndhPjoi0er"},"source":["When getting started with dataframes, the most common question is: *'How do I create a dataframe?'* <br> \n","Below, you can see how to create three kinds of dataframes:"]},{"cell_type":"markdown","metadata":{"id":"QXmRD3hHlM-f"},"source":["### Create a totally empty dataframe"]},{"cell_type":"code","metadata":{"id":"ktkb6s-kjtgG"},"source":["from pyspark.sql.types import StructType\n","sc = spark.sparkContext\n","#Create empty df\n","schema = StructType([])\n","empty = spark.createDataFrame(sc.emptyRDD(), schema)\n","empty.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mg5K3nz_lSDe"},"source":["### Create an empty dataframe with header"]},{"cell_type":"code","metadata":{"id":"9raf4CkRjuTr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617129083352,"user_tz":-330,"elapsed":1044,"user":{"displayName":"Jacob Celestine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCiaSfiqV-4Rwi6vxF7xGxDzuEwgrWpZ_Hcv62gvg=s64","userId":"16466445255655689019"}},"outputId":"afcce8fb-9136-4ec5-91b6-b6924dcea0e1"},"source":["from pyspark.sql.types import StructType, StructField\n","#Create empty df with header\n","schema_header = StructType([StructField(\"name\", StringType(), True)])\n","empty_with_header = spark.createDataFrame(sc.emptyRDD(), schema_header)\n","empty_with_header.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+----+\n","|name|\n","+----+\n","+----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y1ZNOx7ilUnd"},"source":["### Create a dataframe with header and data"]},{"cell_type":"code","metadata":{"id":"TvzyL46QkJBl"},"source":["from pyspark.sql import Row\n","mylist = [\n","  {\"name\":'Alice',\"age\":13},\n","  {\"name\":'Jacob',\"age\":24},\n","  {\"name\":'Betty',\"age\":135},\n","]\n","spark.createDataFrame(Row(**x) for x in mylist).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnRMckA5nLoJ"},"source":["# You can achieve the same using this - note that we are using spark context here, not a spark session\n","from pyspark.sql import Row\n","df = sc.parallelize([\n","        Row(name='Alice', age=13),\n","        Row(name='Jacob', age=24),\n","        Row(name='Betty', age=135)]).toDF()\n","df.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3crkAQVlxKp"},"source":["<a id='drop-duplicates'></a>\n","## Drop Duplicates"]},{"cell_type":"markdown","metadata":{"id":"4IHrYEwHmBcc"},"source":["As mentioned earlier, there are two easy to remove duplicates from a dataframe. We have already seen the usage of distinct under <a href=\"#get-distinct-rows\">Get Distinct Rows</a>  section. \n","I will expalin how to use the `dropDuplicates()` function to achieve the same. \n","\n","> `drop_duplicates()` is an alias for `dropDuplicates()`"]},{"cell_type":"code","metadata":{"id":"wOuHRAPJmWen"},"source":["from pyspark.sql import Row\n","from pyspark.sql import Row\n","mylist = [\n","  {\"name\":'Alice',\"age\":5,\"height\":80},\n","  {\"name\":'Jacob',\"age\":24,\"height\":80},\n","  {\"name\":'Alice',\"age\":5,\"height\":80}\n","]\n","df = spark.createDataFrame(Row(**x) for x in mylist)\n","df.dropDuplicates().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zMv7A-2Hnmjh"},"source":["`dropDuplicates()` can also take in an optional parameter called *subset* which helps specify the columns on which the duplicate check needs to be done on."]},{"cell_type":"code","metadata":{"id":"SHnFylV1n8to"},"source":["df.dropDuplicates(subset=['height']).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZ5xsdWmNOVz"},"source":["<a id='best-practices'></a>\n","### Best Practices"]},{"cell_type":"markdown","metadata":{"id":"NUKLZ8G8NVuR"},"source":["Try to incorporate these to your coding habits for better performance:\n","1.   Do not use NOT IN use NOT EXISTS.\n","2.   Remove Counts, Distinct Counts (use approxCountDIstinct).\n","3.   Drop Duplicates early.\n","4.   Always prefer SQL functions over PandasUDF.\n","5.   Use Hive partitions effectively.\n","6.   Leverage Spark UI effectively. \n","7.   Avoid Shuffle Spills.\n","8.   Aim for target cluster utilization of atleast 70%.\n","\n"]}]}